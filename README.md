# Scaling Applications

## Содержимое registry

![01](images/01.jpg)

## Authenticating with the Registry

![alt text](images/02.jpg)
![alt text](images/03.jpg)
![alt text](images/04.jpg)

## Docker Orchestration Hands-on Lab

![alt text](images/05.png)
![alt text](images/06.png)
![alt text](images/07.png)
![alt text](images/08.png)
![alt text](images/09.png)
![alt text](images/10.png)

> Вопрос: восстановилась ли работа запущенного сервиса на этом узле?

Ответ: нет. После перевода узла из состояния Drain обратно в Active Docker Swarm не переносит задачи сервиса обратно автоматически.

> Что необходимо сделать, чтобы снова запустить сервис на этом узле?

Нужно инициировать перераспределение задач сервиса, или изменить количество реплик.

## Swarm stack introduction

![alt text](images/11.png)
![alt text](images/12.png)
![alt text](images/13.png)

> Как конфигурируется количество нодов в стэке?

Количество экземпляров сервисов в Docker Swarm-стэке задаётся в файле docker-stack.yml с помощью параметра deploy.replicas. Параметр определяет число реплик сервиса, которые планировщик Swarm автоматически распределяет по узлам кластера.

> Как организуется проверка жизнеспособности сервисов?

Контроль работоспособности осуществляется средствами Docker Swarm, который отслеживает состояние задач и автоматически перезапускает контейнеры в случае их отказа, поддерживая заданное количество реплик сервисов.

Docker Swarm поддерживает механизм healthcheck, который может быть описан в docker-stack.yml, например:

```yml
healthcheck:
  test: ["CMD", "curl", "-f", "http://localhost"]
  interval: 30s
  timeout: 10s
  retries: 3
```

## Оценка производительности Flask-приложения при масштабировании в Docker Swarm

![alt text](images/14.png)
![alt text](images/15.png)
![alt text](images/16.png)

Конфигурация теста:

- Инструмент: wrk
- 4 потока, 50 соединений
- Длительность: 30 секунд
- Endpoint: GET /api/counter

| Параметр    | 1 реплика | 4 реплики | Изменение |
| ----------- | --------- | --------- | --------- |
| RPS         | 1695      | 2203      | **+30%**  |
| Avg Latency | 10.61 ms  | 8.65 ms   | **−18%**  |

> Изменился ли потенциал при обработке запросов?

Да, изменился. При увеличении количества инстансов Flask с 1 до 4 – пропускная способность выросла примерно на 30%, а средняя задержка уменьшилась. Это означает, что приложение выигрывает от кластеризованного развертывания, даже в условиях одного хоста.

> Существуют ли какие-то особенности при работе реплицированного сервиса с БД?

Репликация сервиса базы данных без поддержки кластеризации приводит к нарушению целостности данных. В отличие от stateless-сервисов, базы данных требуют специализированных механизмов синхронизации состояния.

## Оценка производительности Flask-приложения при масштабировании в k8s

![alt text](images/17.png)
![alt text](images/18.png)
![alt text](images/19.png)
![alt text](images/20.png)
![alt text](images/21.png)

Конфигурация теста

- Инструмент: wrk
- Потоки: 4
- Соединения: 50
- Длительность: 30 секунд
- Endpoint: GET /api/counter

| Параметр     | 1 реплика | 4 реплики | Изменение       |
| ------------ | --------- | --------- | --------------- |
| Requests/sec | 1309.72   | 1812.60   | **+38%**        |
| Avg Latency  | 14.10 ms  | 14.11 ms  | ~ без изменений |

> Изменился ли потенциал обработки запросов?

Увеличение количества реплик Flask-сервиса с 1 до 4 привело к росту пропускной способности примерно на 38%. Средняя задержка осталась практически неизменной.

> Сравнение с Docker Swarm

Эффект масштабирования в Kubernetes сопоставим с Docker Swarm, однако более сложная сетевая модель Kubernetes приводит к дополнительным накладным расходам и снижению RPS.

### Конфигурационные файлы k8s

1. [counter-deployment.yaml](k8s/counter-deployment.yaml)
2. [counter-service.yaml](k8s/counter-service.yaml)
3. [redis-deployment.yaml](k8s/redis-deployment.yaml)
4. [redis-service.yaml](k8s/redis-service.yaml)
